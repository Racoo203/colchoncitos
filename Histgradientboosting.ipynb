{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\progra.DESKTOP-GV4Q93K\\miniconda3\\envs\\last\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "# ======================================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import  HistGradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se transforman los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_final.csv\")\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0',\"index\",\"Total libre de impuestos\",\"Indefinido total $\",\"Indefinido ctdad\"])\n",
    "df = df.rename(columns={\"date\":\"Fecha\",\"Encoded Products\":\"Producto\"})\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'], format=\"mixed\")\n",
    "df.columns = df.columns.str.replace(\"total $\", \"Precio por unidad\")\n",
    "df.columns = df.columns.str.replace(\"ctdad\", \"Cantidad\")\n",
    "def div(numerator, denominator):\n",
    "  return lambda row: 0.0 if row[denominator] == 0 else float(row[numerator]/row[denominator])\n",
    "for i in range(2, len(df.columns)-1,2):\n",
    "    df[df.columns[i]] = df.apply(div(df.columns[i], df.columns[i+1]), axis=1)\n",
    "#df = df.drop(axis = 0, index = 10865)\n",
    "df = df.set_index('Fecha')\n",
    "df[\"Precio promedio\"] = df.iloc[:,1::2].mean(axis=1)\n",
    "#Decidir si hacerlo antes o despues de elegir variables\n",
    "df[\"Dia\"] = df.index.day\n",
    "df[\"Mes\"] = df.index.month\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Producto 11</th>\n",
       "      <th>Producto 117</th>\n",
       "      <th>Producto 8</th>\n",
       "      <th>Producto 16</th>\n",
       "      <th>Producto 79</th>\n",
       "      <th>Producto 105</th>\n",
       "      <th>Producto 43</th>\n",
       "      <th>Producto 12</th>\n",
       "      <th>Producto 20</th>\n",
       "      <th>Producto 37</th>\n",
       "      <th>...</th>\n",
       "      <th>Producto 208</th>\n",
       "      <th>Producto 206</th>\n",
       "      <th>Producto 207</th>\n",
       "      <th>Producto 205</th>\n",
       "      <th>Producto 204</th>\n",
       "      <th>Producto 203</th>\n",
       "      <th>Producto 202</th>\n",
       "      <th>Producto 201</th>\n",
       "      <th>Producto 272</th>\n",
       "      <th>Producto 273</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-02</th>\n",
       "      <td>10.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>507.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>729 rows × 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Producto 11  Producto 117  Producto 8  Producto 16  Producto 79  \\\n",
       "Fecha                                                                         \n",
       "2022-01-02         10.5           3.5        21.0          3.5          3.5   \n",
       "2022-01-03          3.5           0.0         0.0          3.5          0.0   \n",
       "2022-01-04          7.0           0.0        10.5          3.5          0.0   \n",
       "2022-01-05          0.0           0.0        10.5          3.5          0.0   \n",
       "2022-01-06          0.0           0.0         3.5          7.0          0.0   \n",
       "...                 ...           ...         ...          ...          ...   \n",
       "2023-12-27          0.0           0.0        17.5          3.5          0.0   \n",
       "2023-12-28          0.0           0.0        21.0         10.5          0.0   \n",
       "2023-12-29          0.0           0.0        31.5         21.0          0.0   \n",
       "2023-12-30          0.0           0.0        10.5         17.5          0.0   \n",
       "2023-12-31          0.0           0.0         3.5          7.0          0.0   \n",
       "\n",
       "            Producto 105  Producto 43  Producto 12  Producto 20  Producto 37  \\\n",
       "Fecha                                                                          \n",
       "2022-01-02           3.5          3.5          7.0          3.5          7.0   \n",
       "2022-01-03           0.0          0.0         10.5          0.0          0.0   \n",
       "2022-01-04           0.0          0.0          3.5          3.5          3.5   \n",
       "2022-01-05           0.0          0.0          7.0          0.0          0.0   \n",
       "2022-01-06           0.0          0.0          7.0          0.0          0.0   \n",
       "...                  ...          ...          ...          ...          ...   \n",
       "2023-12-27           0.0          0.0          3.5          0.0          0.0   \n",
       "2023-12-28           0.0          0.0          3.5          3.5          0.0   \n",
       "2023-12-29           0.0          0.0          3.5          0.0          0.0   \n",
       "2023-12-30           0.0          0.0          3.5          0.0          0.0   \n",
       "2023-12-31           0.0          0.0          0.0          3.5          0.0   \n",
       "\n",
       "            ...  Producto 208  Producto 206  Producto 207  Producto 205  \\\n",
       "Fecha       ...                                                           \n",
       "2022-01-02  ...           0.0           0.0           0.0           0.0   \n",
       "2022-01-03  ...           0.0           0.0           0.0           0.0   \n",
       "2022-01-04  ...           0.0           0.0           0.0           0.0   \n",
       "2022-01-05  ...           0.0           0.0           0.0           0.0   \n",
       "2022-01-06  ...           0.0           0.0           0.0           0.0   \n",
       "...         ...           ...           ...           ...           ...   \n",
       "2023-12-27  ...          10.5           0.0           0.0           0.0   \n",
       "2023-12-28  ...           0.0           0.0           0.0           0.0   \n",
       "2023-12-29  ...           3.5           0.0           0.0           0.0   \n",
       "2023-12-30  ...           0.0           0.0           0.0           0.0   \n",
       "2023-12-31  ...           3.5           0.0           0.0           0.0   \n",
       "\n",
       "            Producto 204  Producto 203  Producto 202  Producto 201  \\\n",
       "Fecha                                                                \n",
       "2022-01-02           0.0           0.0           0.0           0.0   \n",
       "2022-01-03           0.0           0.0           0.0           0.0   \n",
       "2022-01-04           0.0           0.0           0.0           0.0   \n",
       "2022-01-05           0.0           0.0           0.0           0.0   \n",
       "2022-01-06           0.0           0.0           0.0           0.0   \n",
       "...                  ...           ...           ...           ...   \n",
       "2023-12-27           0.0           0.0           0.0           0.0   \n",
       "2023-12-28           0.0           0.0           0.0           0.0   \n",
       "2023-12-29           0.0           0.0           0.0           0.0   \n",
       "2023-12-30           0.0           0.0           0.0           0.0   \n",
       "2023-12-31           0.0           0.0           0.0           0.0   \n",
       "\n",
       "            Producto 272  Producto 273  \n",
       "Fecha                                   \n",
       "2022-01-02           0.0           0.0  \n",
       "2022-01-03           0.0           0.0  \n",
       "2022-01-04           0.0           0.0  \n",
       "2022-01-05           0.0           0.0  \n",
       "2022-01-06           0.0           0.0  \n",
       "...                  ...           ...  \n",
       "2023-12-27           0.0         381.5  \n",
       "2023-12-28           0.0         448.0  \n",
       "2023-12-29           0.0         336.0  \n",
       "2023-12-30           0.0         507.5  \n",
       "2023-12-31           0.0         168.0  \n",
       "\n",
       "[729 rows x 274 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for i in df[\"Producto\"].unique():\n",
    "        x =pd.DataFrame(df[df[\"Producto\"]==i].loc[:].groupby(\"Fecha\").sum()[\"Ctdad Ordenada\"].asfreq(\"D\", fill_value=0)).rename(columns={\"Ctdad Ordenada\":i})\n",
    "        data = pd.concat([data,x], axis=1)\n",
    "data.fillna(0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxdf = pd.DataFrame(data.sum(axis=1))\n",
    "boxdf.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se eliminan los datos con menos de 50 ventas en el ultimo año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if data.loc[\"2023-01-01\":,i].sum() < 50:\n",
    "        data = data.drop(columns=i,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Producto 8</th>\n",
       "      <th>Producto 16</th>\n",
       "      <th>Producto 79</th>\n",
       "      <th>Producto 43</th>\n",
       "      <th>Producto 12</th>\n",
       "      <th>Producto 20</th>\n",
       "      <th>Producto 37</th>\n",
       "      <th>Producto 5</th>\n",
       "      <th>Producto 0</th>\n",
       "      <th>Producto 1</th>\n",
       "      <th>...</th>\n",
       "      <th>Producto 182</th>\n",
       "      <th>Producto 150</th>\n",
       "      <th>Producto 122</th>\n",
       "      <th>Producto 148</th>\n",
       "      <th>Producto 131</th>\n",
       "      <th>Producto 134</th>\n",
       "      <th>Producto 208</th>\n",
       "      <th>Producto 201</th>\n",
       "      <th>Producto 272</th>\n",
       "      <th>Producto 273</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-02</th>\n",
       "      <td>21.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>10.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>10.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>49.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>17.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>45.5</td>\n",
       "      <td>45.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>31.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>10.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>507.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>729 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Producto 8  Producto 16  Producto 79  Producto 43  Producto 12  \\\n",
       "Fecha                                                                        \n",
       "2022-01-02        21.0          3.5          3.5          3.5          7.0   \n",
       "2022-01-03         0.0          3.5          0.0          0.0         10.5   \n",
       "2022-01-04        10.5          3.5          0.0          0.0          3.5   \n",
       "2022-01-05        10.5          3.5          0.0          0.0          7.0   \n",
       "2022-01-06         3.5          7.0          0.0          0.0          7.0   \n",
       "...                ...          ...          ...          ...          ...   \n",
       "2023-12-27        17.5          3.5          0.0          0.0          3.5   \n",
       "2023-12-28        21.0         10.5          0.0          0.0          3.5   \n",
       "2023-12-29        31.5         21.0          0.0          0.0          3.5   \n",
       "2023-12-30        10.5         17.5          0.0          0.0          3.5   \n",
       "2023-12-31         3.5          7.0          0.0          0.0          0.0   \n",
       "\n",
       "            Producto 20  Producto 37  Producto 5  Producto 0  Producto 1  ...  \\\n",
       "Fecha                                                                     ...   \n",
       "2022-01-02          3.5          7.0        14.0        10.5        35.0  ...   \n",
       "2022-01-03          0.0          0.0        21.0        38.5        17.5  ...   \n",
       "2022-01-04          3.5          3.5        21.0        56.0         3.5  ...   \n",
       "2022-01-05          0.0          0.0        10.5        49.0        14.0  ...   \n",
       "2022-01-06          0.0          0.0        10.5        17.5         7.0  ...   \n",
       "...                 ...          ...         ...         ...         ...  ...   \n",
       "2023-12-27          0.0          0.0        28.0        45.5        45.5  ...   \n",
       "2023-12-28          3.5          0.0        10.5        28.0        17.5  ...   \n",
       "2023-12-29          0.0          0.0        38.5        35.0        31.5  ...   \n",
       "2023-12-30          0.0          0.0        42.0        56.0        28.0  ...   \n",
       "2023-12-31          3.5          0.0        10.5        35.0         7.0  ...   \n",
       "\n",
       "            Producto 182  Producto 150  Producto 122  Producto 148  \\\n",
       "Fecha                                                                \n",
       "2022-01-02           0.0           0.0           0.0           0.0   \n",
       "2022-01-03           0.0           0.0           0.0           0.0   \n",
       "2022-01-04           0.0           0.0           0.0           0.0   \n",
       "2022-01-05           0.0           0.0           0.0           0.0   \n",
       "2022-01-06           0.0           0.0           0.0           0.0   \n",
       "...                  ...           ...           ...           ...   \n",
       "2023-12-27           0.0           3.5           0.0           3.5   \n",
       "2023-12-28           0.0           0.0           0.0           0.0   \n",
       "2023-12-29           0.0           0.0           0.0           0.0   \n",
       "2023-12-30           0.0           0.0           0.0           0.0   \n",
       "2023-12-31           0.0           0.0           0.0           0.0   \n",
       "\n",
       "            Producto 131  Producto 134  Producto 208  Producto 201  \\\n",
       "Fecha                                                                \n",
       "2022-01-02           0.0           0.0           0.0           0.0   \n",
       "2022-01-03           0.0           0.0           0.0           0.0   \n",
       "2022-01-04           0.0           0.0           0.0           0.0   \n",
       "2022-01-05           0.0           0.0           0.0           0.0   \n",
       "2022-01-06           0.0           0.0           0.0           0.0   \n",
       "...                  ...           ...           ...           ...   \n",
       "2023-12-27           0.0           0.0          10.5           0.0   \n",
       "2023-12-28           0.0           0.0           0.0           0.0   \n",
       "2023-12-29           0.0           0.0           3.5           0.0   \n",
       "2023-12-30           0.0           0.0           0.0           0.0   \n",
       "2023-12-31           0.0           0.0           3.5           0.0   \n",
       "\n",
       "            Producto 272  Producto 273  \n",
       "Fecha                                   \n",
       "2022-01-02           0.0           0.0  \n",
       "2022-01-03           0.0           0.0  \n",
       "2022-01-04           0.0           0.0  \n",
       "2022-01-05           0.0           0.0  \n",
       "2022-01-06           0.0           0.0  \n",
       "...                  ...           ...  \n",
       "2023-12-27           0.0         381.5  \n",
       "2023-12-28           0.0         448.0  \n",
       "2023-12-29           0.0         336.0  \n",
       "2023-12-30           0.0         507.5  \n",
       "2023-12-31           0.0         168.0  \n",
       "\n",
       "[729 rows x 72 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se divide el test en validacion, test y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dates      : 2022-01-02 00:00:00 --- 2023-10-21 00:00:00  (n=658)\n",
      "Validation dates : 2023-10-21 00:00:00 --- 2023-11-30 00:00:00  (n=41)\n",
      "Test dates       : 2023-11-30 00:00:00 --- 2023-12-31 00:00:00  (n=32)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train-validation-test\n",
    "# ======================================================================================\n",
    "end_train = '2023-10-21'\n",
    "end_val = '2023-11-30'\n",
    "\n",
    "data_train = data.loc[:end_train, :].copy()\n",
    "data_val   = data.loc[end_train:end_val, :].copy()\n",
    "data_test  = data.loc[end_val:, :].copy()\n",
    "\n",
    "print(f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\n",
    "print(f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\n",
    "print(f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de los trends que hay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progra.DESKTOP-GV4Q93K\\AppData\\Local\\Temp\\ipykernel_33820\\27394305.py:4: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
      "  data.iloc[:, :4].plot(\n"
     ]
    }
   ],
   "source": [
    "# Plot time series\n",
    "# ======================================================================================\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "data.iloc[:, :4].plot(\n",
    "    legend   = True,\n",
    "    subplots = True, \n",
    "    sharex   = True,\n",
    "    title    = 'Sales of store 2',\n",
    "    ax       = ax, \n",
    ")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partialmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRER BAJO TU PROPIO RIESGO, toma una hora encontrar los mejores hiperparametros para todas las series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 20.571478837970684\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 2.3696279606808597\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 6.621851762226922\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.0031559305581146902\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 12.19729948014503\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 139.31344015965843\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 3.7933055007615493\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.1287347629589712\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.15020416905887665\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.6481919028697061\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.26382388489154185\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 500}\n",
      "  Backtesting metric: 32.285611297232286\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.3533265211557992\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 0.04151046361383327\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 500}\n",
      "  Backtesting metric: 0.0012323023716903744\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 8.258441886111777\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.7852810931462627\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 3.1212061252086545\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.02073039076117524\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.06400075158268693\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 3.2273844324945573\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 2.7747705885114335\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 11.052207200782735\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 12.144921549348245\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.25324795513353227\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.7938725054597761\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.8780859172074098\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.5902033587784048\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 2.0492989541325772\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.0020132915443232\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 4.14427686238374\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.06220411273838815\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.8238345390351303\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.174672622446252\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 3.8697214274163003\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 8.269078948812066\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 5.178353354272638\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.18144684358709623\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.2993029300567108\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 2.778660888820081\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 2.069622311616371\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 3.8526010782749607\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 0.002645448217355913\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 2.0356757212484737\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 1.4573661124163497\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'max_iter': 500}\n",
      "  Backtesting metric: 0.3693209271883685\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 5.915277111178626\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.1891502126974232\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.002867909683381\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.1555821435863114\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'max_iter': 500}\n",
      "  Backtesting metric: 5.380186210480475\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.088358080476143\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 3.0104284617515256\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 2.08642274517799\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'max_iter': 100}\n",
      "  Backtesting metric: 0.33521154705439216\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 2.1069332182922396\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.24317653745282247\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 11.014126066109714\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 0.7641332869932307\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 0.7869351689469847\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 1.1620829037719043\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 8.531386626807098\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'max_iter': 100}\n",
      "  Backtesting metric: 1.487548564102638\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'max_iter': 500}\n",
      "  Backtesting metric: 0.3321172860108171\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.6740414637079819\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_iter': 500}\n",
      "  Backtesting metric: 0.4688395639746166\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 0.006593077293564791\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'max_iter': 500}\n",
      "  Backtesting metric: 5.177690825883246e-08\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'max_iter': 500}\n",
      "  Backtesting metric: 1.08861702448216\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 500}\n",
      "  Backtesting metric: 1.5549538128707832\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6 7] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'max_iter': 100}\n",
      "  Backtesting metric: 0.5799998280682004\n",
      "\n",
      "Number of models compared: 48.\n",
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 500}\n",
      "  Backtesting metric: 4709.546367361596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search and backtesting of each item's model\n",
    "# ======================================================================================\n",
    "items = []\n",
    "mae_values  = []\n",
    "dictes = {}\n",
    "\n",
    "lags_grid = [7, 14, 21]\n",
    "param_grid = {\n",
    "    'max_iter': [100, 500],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "for i, item in enumerate(data.columns):\n",
    "\n",
    "    forecaster = ForecasterAutoreg(\n",
    "                     regressor     = HistGradientBoostingRegressor(random_state=123),\n",
    "                     lags          = 14,\n",
    "                     transformer_y = StandardScaler()\n",
    "                 )\n",
    "\n",
    "    results_grid = grid_search_forecaster(\n",
    "                       forecaster         = forecaster,\n",
    "                       y                  = data.loc[:end_val, item],\n",
    "                       lags_grid          = lags_grid,\n",
    "                       param_grid         = param_grid,\n",
    "                       steps              = 7,\n",
    "                       metric             = 'mean_squared_error',\n",
    "                       initial_train_size = len(data_train),\n",
    "                       refit              = False,\n",
    "                       fixed_train_size   = False,\n",
    "                       return_best        = True,\n",
    "                       verbose            = False,\n",
    "                       show_progress      = False \n",
    "                  )\n",
    "\n",
    "    metric, preds = backtesting_forecaster(\n",
    "                        forecaster         = forecaster,\n",
    "                        y                  = data[item],\n",
    "                        initial_train_size = len(data_train) + len(data_val),\n",
    "                        steps              = 7,\n",
    "                        metric             = ['mean_squared_error', 'mean_absolute_percentage_error'],\n",
    "                        refit              = False,\n",
    "                        fixed_train_size   = False,\n",
    "                        verbose            = False,\n",
    "                        show_progress      = False\n",
    "                    )\n",
    "\n",
    "    items.append(item)\n",
    "    mae_values.append(metric)\n",
    "    dictes[item] = results_grid\n",
    "    \n",
    "\n",
    "uni_series_mae = pd.Series(\n",
    "                     data  = mae_values,\n",
    "                     index = items,\n",
    "                     name  = 'uni_series_mae'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se procede a hacer un modelo para cada uno de ellos. Será necesario obtener las medidas de error para cada uno de ellos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m uni_series_mae[\u001b[43muni_series_mae\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m]\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[1;32mc:\\Users\\progra.DESKTOP-GV4Q93K\\miniconda3\\envs\\last\\lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\progra.DESKTOP-GV4Q93K\\miniconda3\\envs\\last\\lib\\site-packages\\pandas\\core\\arraylike.py:48\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__lt__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__lt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\progra.DESKTOP-GV4Q93K\\miniconda3\\envs\\last\\lib\\site-packages\\pandas\\core\\series.py:5803\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5800\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   5801\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 5803\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\progra.DESKTOP-GV4Q93K\\miniconda3\\envs\\last\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\progra.DESKTOP-GV4Q93K\\miniconda3\\envs\\last\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:131\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mops.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "uni_series_mae[uni_series_mae < 1].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esos csv estan los parametros y los resultados de MAPE que dieron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dictes.keys():\n",
    "    dictes[i].to_csv(f\"dictes_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>2.069622</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>2.098394</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>2.323072</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>2.458986</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>2.538598</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>2.635012</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>2.847323</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>2.878149</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>2.940070</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>2.964621</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>3.026502</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>3.072858</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>3.086603</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>3.128590</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>3.136751</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>3.138584</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>3.152316</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>3.160134</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>3.160361</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>3.167514</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>3.177134</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>3.185401</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>3.206132</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>3.206701</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>3.209519</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>3.209746</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>3.231532</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>3.232152</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>3.233428</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>3.235918</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>3.237858</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>3.245107</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>3.266772</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>3.266772</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>3.277470</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>3.277470</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>3.279603</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>3.281316</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>3.285682</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>3.285682</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>3.285964</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>3.287238</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>3.290462</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>3.290462</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>3.290978</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>3.296304</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>3.298546</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>3.305234</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 lags  \\\n",
       "47  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "45  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "31    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "29    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "43  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "27    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "41  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "25    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "44  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "46  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "28    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "30    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "42  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "26    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "23    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "24    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "21    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "37  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "40  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "39  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "19    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "17    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "36  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "35  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "38  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "33  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "34  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "16    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "18    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "22    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "20    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "32  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "7                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "5                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "14                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "12                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "1                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "3                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "6                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "4                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "11                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "8                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "15                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "13                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "0                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "2                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "9                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "10                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "\n",
       "                                               params  mean_squared_error  \\\n",
       "47  {'learning_rate': 0.1, 'max_depth': None, 'max...            2.069622   \n",
       "45  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...            2.098394   \n",
       "31  {'learning_rate': 0.1, 'max_depth': None, 'max...            2.323072   \n",
       "29  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...            2.458986   \n",
       "43  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...            2.538598   \n",
       "27  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...            2.635012   \n",
       "41  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...            2.847323   \n",
       "25  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...            2.878149   \n",
       "44  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...            2.940070   \n",
       "46  {'learning_rate': 0.1, 'max_depth': None, 'max...            2.964621   \n",
       "28  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...            3.026502   \n",
       "30  {'learning_rate': 0.1, 'max_depth': None, 'max...            3.072858   \n",
       "42  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...            3.086603   \n",
       "26  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...            3.128590   \n",
       "23  {'learning_rate': 0.01, 'max_depth': None, 'ma...            3.136751   \n",
       "24  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...            3.138584   \n",
       "21  {'learning_rate': 0.01, 'max_depth': 10, 'max_...            3.152316   \n",
       "37  {'learning_rate': 0.01, 'max_depth': 10, 'max_...            3.160134   \n",
       "40  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...            3.160361   \n",
       "39  {'learning_rate': 0.01, 'max_depth': None, 'ma...            3.167514   \n",
       "19  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...            3.177134   \n",
       "17  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...            3.185401   \n",
       "36  {'learning_rate': 0.01, 'max_depth': 10, 'max_...            3.206132   \n",
       "35  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...            3.206701   \n",
       "38  {'learning_rate': 0.01, 'max_depth': None, 'ma...            3.209519   \n",
       "33  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...            3.209746   \n",
       "34  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...            3.231532   \n",
       "16  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...            3.232152   \n",
       "18  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...            3.233428   \n",
       "22  {'learning_rate': 0.01, 'max_depth': None, 'ma...            3.235918   \n",
       "20  {'learning_rate': 0.01, 'max_depth': 10, 'max_...            3.237858   \n",
       "32  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...            3.245107   \n",
       "7   {'learning_rate': 0.01, 'max_depth': None, 'ma...            3.266772   \n",
       "5   {'learning_rate': 0.01, 'max_depth': 10, 'max_...            3.266772   \n",
       "14  {'learning_rate': 0.1, 'max_depth': None, 'max...            3.277470   \n",
       "12  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...            3.277470   \n",
       "1   {'learning_rate': 0.01, 'max_depth': 3, 'max_i...            3.279603   \n",
       "3   {'learning_rate': 0.01, 'max_depth': 5, 'max_i...            3.281316   \n",
       "6   {'learning_rate': 0.01, 'max_depth': None, 'ma...            3.285682   \n",
       "4   {'learning_rate': 0.01, 'max_depth': 10, 'max_...            3.285682   \n",
       "11  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...            3.285964   \n",
       "8   {'learning_rate': 0.1, 'max_depth': 3, 'max_it...            3.287238   \n",
       "15  {'learning_rate': 0.1, 'max_depth': None, 'max...            3.290462   \n",
       "13  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...            3.290462   \n",
       "0   {'learning_rate': 0.01, 'max_depth': 3, 'max_i...            3.290978   \n",
       "2   {'learning_rate': 0.01, 'max_depth': 5, 'max_i...            3.296304   \n",
       "9   {'learning_rate': 0.1, 'max_depth': 3, 'max_it...            3.298546   \n",
       "10  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...            3.305234   \n",
       "\n",
       "    learning_rate  max_depth  max_iter  \n",
       "47           0.10        NaN     500.0  \n",
       "45           0.10       10.0     500.0  \n",
       "31           0.10        NaN     500.0  \n",
       "29           0.10       10.0     500.0  \n",
       "43           0.10        5.0     500.0  \n",
       "27           0.10        5.0     500.0  \n",
       "41           0.10        3.0     500.0  \n",
       "25           0.10        3.0     500.0  \n",
       "44           0.10       10.0     100.0  \n",
       "46           0.10        NaN     100.0  \n",
       "28           0.10       10.0     100.0  \n",
       "30           0.10        NaN     100.0  \n",
       "42           0.10        5.0     100.0  \n",
       "26           0.10        5.0     100.0  \n",
       "23           0.01        NaN     500.0  \n",
       "24           0.10        3.0     100.0  \n",
       "21           0.01       10.0     500.0  \n",
       "37           0.01       10.0     500.0  \n",
       "40           0.10        3.0     100.0  \n",
       "39           0.01        NaN     500.0  \n",
       "19           0.01        5.0     500.0  \n",
       "17           0.01        3.0     500.0  \n",
       "36           0.01       10.0     100.0  \n",
       "35           0.01        5.0     500.0  \n",
       "38           0.01        NaN     100.0  \n",
       "33           0.01        3.0     500.0  \n",
       "34           0.01        5.0     100.0  \n",
       "16           0.01        3.0     100.0  \n",
       "18           0.01        5.0     100.0  \n",
       "22           0.01        NaN     100.0  \n",
       "20           0.01       10.0     100.0  \n",
       "32           0.01        3.0     100.0  \n",
       "7            0.01        NaN     500.0  \n",
       "5            0.01       10.0     500.0  \n",
       "14           0.10        NaN     100.0  \n",
       "12           0.10       10.0     100.0  \n",
       "1            0.01        3.0     500.0  \n",
       "3            0.01        5.0     500.0  \n",
       "6            0.01        NaN     100.0  \n",
       "4            0.01       10.0     100.0  \n",
       "11           0.10        5.0     500.0  \n",
       "8            0.10        3.0     100.0  \n",
       "15           0.10        NaN     500.0  \n",
       "13           0.10       10.0     500.0  \n",
       "0            0.01        3.0     100.0  \n",
       "2            0.01        5.0     100.0  \n",
       "9            0.10        3.0     500.0  \n",
       "10           0.10        5.0     100.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictes[\"Producto 101\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdictes\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProducto 0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m47\u001b[39m][\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictes' is not defined"
     ]
    }
   ],
   "source": [
    "dic0 = pd.read_csv(\"0.csv\")\n",
    "dic273 = pd.read_csv(\"273.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>0.123255</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>0.179854</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>0.201374</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>0.209886</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>0.229158</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>0.232482</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>0.245357</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>0.250662</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>0.385138</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>0.386888</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>0.391043</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>0.405075</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>0.425701</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>0.426555</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>0.437623</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>0.455076</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>0.456435</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>0.456435</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>0.456834</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>0.456834</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>0.459977</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>0.464559</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>0.469658</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>0.471551</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>0.472785</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>0.473565</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>0.473600</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>0.474345</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>0.478634</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>0.492773</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>0.503449</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>0.508657</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>0.521165</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': None, 'max...</td>\n",
       "      <td>0.523120</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>0.523533</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>0.528360</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>0.532705</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>0.538160</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'max_it...</td>\n",
       "      <td>0.553737</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>0.566490</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 10, 'max_i...</td>\n",
       "      <td>0.568864</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': None, 'ma...</td>\n",
       "      <td>0.572274</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 10, 'max_...</td>\n",
       "      <td>0.573660</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>0.574705</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'max_i...</td>\n",
       "      <td>0.579119</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'max_it...</td>\n",
       "      <td>0.582163</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'max_i...</td>\n",
       "      <td>0.597166</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 lags  \\\n",
       "43  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "47  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "31    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "45  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "27    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "29    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "41  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "25    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "13                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "15                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "28    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "30    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "46  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "26    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "44  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "24    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "36  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "38  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "20    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "22    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "34  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "18    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "11                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "32  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "2                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "6                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "4                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "16    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "0                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "9                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "23    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "17    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "21    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "42  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "14                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "40  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "39  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "19    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "37  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "8                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "33  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "12                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "7                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "5                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "35  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "1                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "10                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "3                               [1, 2, 3, 4, 5, 6, 7]   \n",
       "\n",
       "                                               params  \\\n",
       "43  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...   \n",
       "47  {'learning_rate': 0.1, 'max_depth': None, 'max...   \n",
       "31  {'learning_rate': 0.1, 'max_depth': None, 'max...   \n",
       "45  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...   \n",
       "27  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...   \n",
       "29  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...   \n",
       "41  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...   \n",
       "25  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...   \n",
       "13  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...   \n",
       "15  {'learning_rate': 0.1, 'max_depth': None, 'max...   \n",
       "28  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...   \n",
       "30  {'learning_rate': 0.1, 'max_depth': None, 'max...   \n",
       "46  {'learning_rate': 0.1, 'max_depth': None, 'max...   \n",
       "26  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...   \n",
       "44  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...   \n",
       "24  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...   \n",
       "36  {'learning_rate': 0.01, 'max_depth': 10, 'max_...   \n",
       "38  {'learning_rate': 0.01, 'max_depth': None, 'ma...   \n",
       "20  {'learning_rate': 0.01, 'max_depth': 10, 'max_...   \n",
       "22  {'learning_rate': 0.01, 'max_depth': None, 'ma...   \n",
       "34  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...   \n",
       "18  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...   \n",
       "11  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...   \n",
       "32  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...   \n",
       "2   {'learning_rate': 0.01, 'max_depth': 5, 'max_i...   \n",
       "6   {'learning_rate': 0.01, 'max_depth': None, 'ma...   \n",
       "4   {'learning_rate': 0.01, 'max_depth': 10, 'max_...   \n",
       "16  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...   \n",
       "0   {'learning_rate': 0.01, 'max_depth': 3, 'max_i...   \n",
       "9   {'learning_rate': 0.1, 'max_depth': 3, 'max_it...   \n",
       "23  {'learning_rate': 0.01, 'max_depth': None, 'ma...   \n",
       "17  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...   \n",
       "21  {'learning_rate': 0.01, 'max_depth': 10, 'max_...   \n",
       "42  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...   \n",
       "14  {'learning_rate': 0.1, 'max_depth': None, 'max...   \n",
       "40  {'learning_rate': 0.1, 'max_depth': 3, 'max_it...   \n",
       "39  {'learning_rate': 0.01, 'max_depth': None, 'ma...   \n",
       "19  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...   \n",
       "37  {'learning_rate': 0.01, 'max_depth': 10, 'max_...   \n",
       "8   {'learning_rate': 0.1, 'max_depth': 3, 'max_it...   \n",
       "33  {'learning_rate': 0.01, 'max_depth': 3, 'max_i...   \n",
       "12  {'learning_rate': 0.1, 'max_depth': 10, 'max_i...   \n",
       "7   {'learning_rate': 0.01, 'max_depth': None, 'ma...   \n",
       "5   {'learning_rate': 0.01, 'max_depth': 10, 'max_...   \n",
       "35  {'learning_rate': 0.01, 'max_depth': 5, 'max_i...   \n",
       "1   {'learning_rate': 0.01, 'max_depth': 3, 'max_i...   \n",
       "10  {'learning_rate': 0.1, 'max_depth': 5, 'max_it...   \n",
       "3   {'learning_rate': 0.01, 'max_depth': 5, 'max_i...   \n",
       "\n",
       "    mean_absolute_percentage_error  learning_rate  max_depth  max_iter  \n",
       "43                        0.123255           0.10        5.0     500.0  \n",
       "47                        0.179854           0.10        NaN     500.0  \n",
       "31                        0.201374           0.10        NaN     500.0  \n",
       "45                        0.209886           0.10       10.0     500.0  \n",
       "27                        0.229158           0.10        5.0     500.0  \n",
       "29                        0.232482           0.10       10.0     500.0  \n",
       "41                        0.245357           0.10        3.0     500.0  \n",
       "25                        0.250662           0.10        3.0     500.0  \n",
       "13                        0.385138           0.10       10.0     500.0  \n",
       "15                        0.386888           0.10        NaN     500.0  \n",
       "28                        0.391043           0.10       10.0     100.0  \n",
       "30                        0.405075           0.10        NaN     100.0  \n",
       "46                        0.425701           0.10        NaN     100.0  \n",
       "26                        0.426555           0.10        5.0     100.0  \n",
       "44                        0.437623           0.10       10.0     100.0  \n",
       "24                        0.455076           0.10        3.0     100.0  \n",
       "36                        0.456435           0.01       10.0     100.0  \n",
       "38                        0.456435           0.01        NaN     100.0  \n",
       "20                        0.456834           0.01       10.0     100.0  \n",
       "22                        0.456834           0.01        NaN     100.0  \n",
       "34                        0.459977           0.01        5.0     100.0  \n",
       "18                        0.464559           0.01        5.0     100.0  \n",
       "11                        0.469658           0.10        5.0     500.0  \n",
       "32                        0.471551           0.01        3.0     100.0  \n",
       "2                         0.472785           0.01        5.0     100.0  \n",
       "6                         0.473565           0.01        NaN     100.0  \n",
       "4                         0.473600           0.01       10.0     100.0  \n",
       "16                        0.474345           0.01        3.0     100.0  \n",
       "0                         0.478634           0.01        3.0     100.0  \n",
       "9                         0.492773           0.10        3.0     500.0  \n",
       "23                        0.502928           0.01        NaN     500.0  \n",
       "17                        0.503449           0.01        3.0     500.0  \n",
       "21                        0.508657           0.01       10.0     500.0  \n",
       "42                        0.521165           0.10        5.0     100.0  \n",
       "14                        0.523120           0.10        NaN     100.0  \n",
       "40                        0.523533           0.10        3.0     100.0  \n",
       "39                        0.528360           0.01        NaN     500.0  \n",
       "19                        0.532705           0.01        5.0     500.0  \n",
       "37                        0.538160           0.01       10.0     500.0  \n",
       "8                         0.553737           0.10        3.0     100.0  \n",
       "33                        0.566490           0.01        3.0     500.0  \n",
       "12                        0.568864           0.10       10.0     100.0  \n",
       "7                         0.572274           0.01        NaN     500.0  \n",
       "5                         0.573660           0.01       10.0     500.0  \n",
       "35                        0.574705           0.01        5.0     500.0  \n",
       "1                         0.579119           0.01        3.0     500.0  \n",
       "10                        0.582163           0.10        5.0     100.0  \n",
       "3                         0.597166           0.01        5.0     500.0  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictes[\"Producto 273\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asi se hace un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=HistGradientBoostingRegressor(random_state=123, max_iter=500, max_depth=10, learning_rate=0.1),\n",
    "    lags=14,\n",
    "    transformer_y=StandardScaler()\n",
    ")                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como se ve el producto 0 con su predicción y los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster.fit(y = data_train[\"Producto 0\"].loc[:end_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fecha\n",
       "2023-11-22    21.0\n",
       "2023-11-23    10.5\n",
       "2023-11-24    10.5\n",
       "2023-11-25    52.5\n",
       "2023-11-26    21.0\n",
       "2023-11-27    45.5\n",
       "2023-11-28    14.0\n",
       "Freq: D, Name: Producto 0, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Producto 0\"].loc[\"2023-11-22\":\"2023-11-28\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster.predict(50).loc[\"2023-11-22\":\"2023-11-28\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar modelo del otro producto y comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=forecaster.predict(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = data[\"Producto 0\"].loc[\"2023-10-21\":\"2023-10-28\"] - predicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
